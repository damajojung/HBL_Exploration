{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualistion of Learning NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import pathlib\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "n_c = 10 # Number of classes\n",
    "dims = 300 # Dimension\n",
    "samp_size = 1000 # Sample Size\n",
    "test_prop = 0.2 # Test Proportion\n",
    "\n",
    "path = str(pathlib.Path().resolve())\n",
    "path_train = path + '/syn_data' + '/' + str(n_c) + 'c' + str(dims) + 'd' + str(samp_size) + 's' + '_train' + '.csv'\n",
    "path_test = path + '/syn_data' + '/' + str(n_c) + 'c' + str(dims) + 'd' + str(samp_size)+ 's' + '_test' + '.csv'\n",
    "train = pd.read_csv(path_train, index_col=0)\n",
    "test = pd.read_csv(path_test, index_col=0)\n",
    "\n",
    "def csv_torch(df):\n",
    "\n",
    "    # Creating Tuples\n",
    "    cols = (df.columns)\n",
    "    rows = np.unique(df.index)\n",
    "\n",
    "    samps = []\n",
    "\n",
    "    for i in rows:\n",
    "        s  = df.loc[int(i),:]\n",
    "        for j in cols:\n",
    "            t = torch.Tensor(np.array(s[str(j)]))\n",
    "            tup = (t, int(i))\n",
    "            samps.append(tup)\n",
    "    return samps\n",
    "\n",
    "dat_train = csv_torch(train)\n",
    "dat_test = csv_torch(test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(dat_train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(dat_test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dims, 300) # Input Layer\n",
    "        self.fc2 = nn.Linear(300, 300) # Hidden Layer 1\n",
    "        self.fc3 = nn.Linear(300, 300) # Hidden Layer 2\n",
    "        self.fc4 = nn.Linear(300, n_c) # Output Layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1) # We want to sum the classes - across columns\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss() # nn.MSELoss() <- is also an option\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5493, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0408, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0095, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS): # 3 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,dims))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(data):\n",
    "    target =  [] \n",
    "    preds =  [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data\n",
    "            output = net(X.view(-1,dims))\n",
    "            for idx, i in enumerate(output):\n",
    "                preds.append(int(torch.argmax(i)))\n",
    "                target.append(int(y[idx]))\n",
    "\n",
    "    preds = torch.tensor(preds)\n",
    "    target = torch.tensor(target)\n",
    "\n",
    "    accuracy = Accuracy()\n",
    "    a = accuracy(preds, target)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, train = False): # By default, we will not update weights\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "\n",
    "    outputs = net(X)\n",
    "    matches = [torch.argmax(i) == torch.argmax(j) for i,j in zip(outputs,y)] # Comparng the argmax of both of those vectors when they are the same, it will be true, otherwise false\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return acc, loss\n",
    "\n",
    "def test(data, size=32):\n",
    "    test_X, test_y = data[0], data[1]\n",
    "    random_start = np.random.randint(len(test_X) - size)\n",
    "    X, y = test_X[random_start:random_start+size], test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = fwd_pass(X.view(-1, dims),y) #.to(device), y.to(device) for cloud computing\n",
    "    return val_acc, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f3e38fe13f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss),4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2f3e38fe13f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train, trest, net)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "\n",
    "net = Net() # .to(device) # <- for cloud GPU\n",
    "\n",
    "def train(train, trest, net):\n",
    "    train_X, train_y = train\n",
    "    test_X, train_y = test\n",
    "    BATCH_SIZE = 10\n",
    "    EPOCHS = 3\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE] # .view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X, batch_y # batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                val_acc, val_loss = test(size = 100)\n",
    "                print(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss),4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
    "\n",
    "train(trainset, testset, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCHS): # 3 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,dims))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
