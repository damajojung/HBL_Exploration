{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codebase from hyperspherical prototype networks, Pascal Mettes, NeurIPS2019\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"classification\")\n",
    "    parser.add_argument(\"--data_name\", dest=\"data_name\", default=\"cifar100\",\n",
    "                        choices=[\"cifar100\", \"cifar10\", \"cub\"], type=str)  # choose tha name of the dataset\n",
    "\n",
    "    parser.add_argument(\"--datadir\", dest=\"datadir\", default=\"dat/\", type=str)\n",
    "    parser.add_argument(\"--resdir\", dest=\"resdir\", default=\"res/\", type=str)\n",
    "    parser.add_argument(\"--hpnfile\", dest=\"hpnfile\", default=\"\", type=str)\n",
    "    parser.add_argument(\"--logdir\", dest=\"logdir\", default=\"\", type=str)\n",
    "    parser.add_argument(\"--loss\", dest=\"loss_name\", default=\"PeBuseLoss\", type=str)\n",
    "\n",
    "    parser.add_argument(\"-n\", dest=\"network\", default=\"resnet32\", type=str)\n",
    "    parser.add_argument(\"-r\", dest=\"optimizer\", default=\"sgd\", type=str)\n",
    "    parser.add_argument(\"-l\", dest=\"learning_rate\", default=0.01, type=float)\n",
    "    parser.add_argument(\"-m\", dest=\"momentum\", default=0.9, type=float)\n",
    "    parser.add_argument(\"-c\", dest=\"decay\", default=0.0001, type=float)\n",
    "    parser.add_argument(\"-s\", dest=\"batch_size\", default=128, type=int)\n",
    "    parser.add_argument(\"-e\", dest=\"epochs\", default=250, type=int)\n",
    "    parser.add_argument(\"-p\", dest=\"penalty\", default='dim', type=str)  # choose penalty in loss\n",
    "    parser.add_argument(\"--mult\", dest=\"mult\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--curv\", dest=\"curv\", default=1.0, type=float)\n",
    "\n",
    "    parser.add_argument(\"--seed\", dest=\"seed\", default=100, type=int)\n",
    "    parser.add_argument(\"--drop1\", dest=\"drop1\", default=500, type=int)\n",
    "    parser.add_argument(\"--drop2\", dest=\"drop2\", default=1000, type=int)\n",
    "    parser.add_argument(\"--do_decay\", dest=\"do_decay\", default=False, type=bool)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# General helpers.\n",
    "################################################################################\n",
    "\n",
    "#\n",
    "# Count the number of learnable parameters in a model.\n",
    "#\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "#\n",
    "# Get the desired optimizer.\n",
    "#\n",
    "def get_optimizer(optimname, params, learning_rate, momentum, decay):\n",
    "    if optimname == \"sgd\":\n",
    "        optimizer = optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=decay)\n",
    "    elif optimname == \"adadelta\":\n",
    "        optimizer = optim.Adadelta(params, lr=learning_rate, weight_decay=decay)\n",
    "    elif optimname == \"adam\":\n",
    "        optimizer = optim.Adam(params, lr=learning_rate, weight_decay=decay)\n",
    "    elif optimname == \"adamW\":\n",
    "        optimizer = optim.AdamW(params, lr=learning_rate, weight_decay=decay)\n",
    "    elif optimname == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(params, lr=learning_rate, weight_decay=decay, momentum=momentum)\n",
    "    elif optimname == \"asgd\":\n",
    "        optimizer = optim.ASGD(params, lr=learning_rate, weight_decay=decay)\n",
    "    elif optimname == \"adamax\":\n",
    "        optimizer = optim.Adamax(params, lr=learning_rate, weight_decay=decay)\n",
    "    else:\n",
    "        print('Your option for the optimizer is not available, I am loading SGD.')\n",
    "        optimizer = optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=decay)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Standard dataset loaders.\n",
    "################################################################################\n",
    "def load_dataset(dataset_name, basedir, batch_size, kwargs): #Â kwargs = {'num_workers': 64, 'pin_memory': True}\n",
    "    if dataset_name == 'cifar100':\n",
    "        return load_cifar100(basedir, batch_size, kwargs) # that is mine\n",
    "    else:\n",
    "        raise Exception('Selected dataset is not available.')\n",
    "\n",
    "        # I gues they are working with a map-style dataset since the compute the len\n",
    "\n",
    "\n",
    "def load_cifar100(basedir, batch_size, kwargs): # That is mine as well\n",
    "    # Input channels normalization.\n",
    "    mrgb = [0.507, 0.487, 0.441]\n",
    "    srgb = [0.267, 0.256, 0.276]\n",
    "    normalize = transforms.Normalize(mean=mrgb, std=srgb)\n",
    "\n",
    "    # Load train data.\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root=basedir + 'cifar100/', train=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.RandomCrop(32, 4), # Crop randomly the image in a sample\n",
    "                              transforms.RandomHorizontalFlip(),\n",
    "                              transforms.ToTensor(), #  to convert the numpy images to torch images (we need to swap axes)\n",
    "                              normalize,\n",
    "                          ]), download=True),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # Labels to torch.\n",
    "    trainloader.dataset.train_labels = torch.from_numpy(np.array(trainloader.dataset.targets))\n",
    "\n",
    "    # Load test data.\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root=basedir + 'cifar100/', train=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              normalize,\n",
    "                          ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # Labels to torch.\n",
    "    testloader.dataset.test_labels = torch.from_numpy(np.array(testloader.dataset.targets))\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 64, 'pin_memory': True}\n",
    "batch_size = 128\n",
    "basedir = '/Users/dj/Documents/GitHub/Master_Thesis/Code/HBL_GPU/data/'\n",
    "data_name = 'cifar100'\n",
    "\n",
    "def load_cifar100(basedir, batch_size, kwargs): # That is mine as well\n",
    "    # Input channels normalization.\n",
    "    mrgb = [0.507, 0.487, 0.441]\n",
    "    srgb = [0.267, 0.256, 0.276]\n",
    "    normalize = transforms.Normalize(mean=mrgb, std=srgb)\n",
    "\n",
    "    # Load train data.\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root=basedir + 'cifar100/', train=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.RandomCrop(32, 4), # Crop randomly the image in a sample\n",
    "                              transforms.RandomHorizontalFlip(),\n",
    "                              transforms.ToTensor(), #  to convert the numpy images to torch images (we need to swap axes)\n",
    "                              normalize,\n",
    "                          ]), download=True),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # Labels to torch.\n",
    "    trainloader.dataset.train_labels = torch.from_numpy(np.array(trainloader.dataset.targets))\n",
    "\n",
    "    # Load test data.\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root=basedir + 'cifar100/', train=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              normalize,\n",
    "                          ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # Labels to torch.\n",
    "    testloader.dataset.test_labels = torch.from_numpy(np.array(testloader.dataset.targets))\n",
    "\n",
    "    return trainloader, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 64, 'pin_memory': True}\n",
    "batch_size = 128\n",
    "basedir = '/Users/dj/Documents/GitHub/Master_Thesis/Code/HBL_GPU/data/'\n",
    "data_name = 'cifar100'\n",
    "args = ['cifar100', basedir, batch_size]\n",
    "\n",
    "trainloader, testloader = load_cifar100(basedir, batch_size, kwargs) \n",
    "# trainloader, testloader = load_dataset('cifar100', basedir, batch_size, kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.8989, -1.8989, -1.8989,  ..., -1.8989, -1.8989, -1.8989],\n",
       "          [-1.8989, -1.8989, -1.8989,  ..., -1.8989, -1.8989, -1.8989],\n",
       "          [-1.8989, -1.8989, -1.8989,  ..., -1.8989, -1.8989, -1.8989],\n",
       "          ...,\n",
       "          [-1.8989, -0.3714, -0.6798,  ...,  0.5393,  0.5686,  0.5393],\n",
       "          [-1.8989, -0.5036, -0.7092,  ...,  0.7302,  0.7743,  0.6127],\n",
       "          [-1.8989, -0.7239, -0.8707,  ...,  0.4658,  0.4218,  0.4511]],\n",
       " \n",
       "         [[-1.9023, -1.9023, -1.9023,  ..., -1.9023, -1.9023, -1.9023],\n",
       "          [-1.9023, -1.9023, -1.9023,  ..., -1.9023, -1.9023, -1.9023],\n",
       "          [-1.9023, -1.9023, -1.9023,  ..., -1.9023, -1.9023, -1.9023],\n",
       "          ...,\n",
       "          [-1.9023, -0.5390, -0.8913,  ...,  1.2227,  1.2686,  1.2227],\n",
       "          [-1.9023, -0.6003, -0.8760,  ...,  1.4065,  1.4678,  1.3299],\n",
       "          [-1.9023, -0.7534, -0.9832,  ...,  1.1614,  1.1461,  1.1920]],\n",
       " \n",
       "         [[-1.5978, -1.5978, -1.5978,  ..., -1.5978, -1.5978, -1.5978],\n",
       "          [-1.5978, -1.5978, -1.5978,  ..., -1.5978, -1.5978, -1.5978],\n",
       "          [-1.5978, -1.5978, -1.5978,  ..., -1.5978, -1.5978, -1.5978],\n",
       "          ...,\n",
       "          [-1.5978, -0.7311, -1.1005,  ..., -0.3475, -0.2622, -0.2906],\n",
       "          [-1.5978, -0.9300, -1.1716,  ..., -0.0917, -0.0349, -0.2906],\n",
       "          [-1.5978, -1.2284, -1.3563,  ..., -0.4753, -0.6174, -0.6885]]]),\n",
       " 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0491)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(testloader.dataset[3][0])[2][0][0]\n",
    "\n",
    "# type dataset[0] = tuple  -> First entry is a Torch Tensor, the second one an int\n",
    "# shape = torch.Size([3, 32, 32]) # torch image: C x H x W\n",
    "# type = torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100\n",
       "    Number of datapoints: 50000\n",
       "    Root location: /Users/dj/Documents/GitHub/Master_Thesis/Code/HBL_GPU/data/cifar100/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomCrop(size=(32, 32), padding=4)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
       "           )"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8cdf8e27a6a01ad1cac13f7a3302b334775911623636020b5b45a311449ddcb2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python_env_37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
