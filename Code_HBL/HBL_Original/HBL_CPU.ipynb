{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from helper import pmath\n",
    "from helper.helper import get_optimizer, load_dataset \n",
    "from helper.hyperbolicLoss import PeBusePenalty\n",
    "from models.cifar import resnet as resnet_cifar\n",
    "from models.cifar import densenet as densenet_cifar\n",
    "from models.cub import resnet as resnet_cub\n",
    "# from models.syndat import fullcon as fullcon_syndat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(model, trainloader, optimizer, initialized_loss, c=1.0):\n",
    "    # Set mode to training.\n",
    "    model.train()\n",
    "    avgloss, avglosscount, newloss, acc, newacc = 0., 0., 0., 0., 0.\n",
    "\n",
    "    # Go over all batches.\n",
    "    for bidx, (data, target) in enumerate(trainloader):\n",
    "        # bidx: Number of batches -> reaches from 0 - len(trainloader)\n",
    "        # dara: btach_size tensors\n",
    "        # Target: A tensor containing numbers from 0-100 (sice its cifar 100, I assume these\n",
    "        # are the numbers of the class - basically the target and the y in my code)\n",
    "\n",
    "        # Data to device.\n",
    "        target_tmp = target # .cuda()\n",
    "\n",
    "        target = model.polars[target]\n",
    "        data = torch.autograd.Variable(data) # .cuda()\n",
    "        target = torch.autograd.Variable(target) # .cuda()\n",
    "        # Compute outputs and losses.\n",
    "        output = model(data)\n",
    "        output_exp_map = pmath.expmap0(output, c=c)\n",
    "\n",
    "        loss_function = initialized_loss(output_exp_map, target)\n",
    "\n",
    "        # Backpropagation.\n",
    "        optimizer.zero_grad()\n",
    "        loss_function.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avgloss += loss_function.item()\n",
    "        avglosscount += 1.\n",
    "        newloss = avgloss / avglosscount\n",
    "\n",
    "        output = model.predict(output_exp_map).float()\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        acc += pred.eq(target_tmp.view_as(pred)).sum().item()\n",
    "\n",
    "    trainlen = len(trainloader.dataset)\n",
    "    newacc = acc / float(trainlen)\n",
    "\n",
    "    # I am returning new loss to show in the tensorboard!\n",
    "    return newacc, newloss\n",
    "\n",
    "\n",
    "def main_test(model, testloader, initialized_loss, c=1.0):\n",
    "    # Set model to evaluation and initialize accuracy and cosine similarity.\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "\n",
    "    # Go over all batches.\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            # Data to device.\n",
    "            data = torch.autograd.Variable(data) # .cuda()\n",
    "            target = target.cuda(non_blocking=True)\n",
    "            target = torch.autograd.Variable(target)\n",
    "            target_loss = model.polars[target]\n",
    "\n",
    "            # Forward.\n",
    "            output = model(data).float()\n",
    "            output_exp_map = pmath.expmap0(output, c=c)\n",
    "\n",
    "            output = model.predict(output_exp_map).float()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            loss += initialized_loss(output_exp_map, target_loss) # .cuda())\n",
    "\n",
    "    # Print results.\n",
    "    testlen = len(testloader.dataset)\n",
    "\n",
    "    avg_acc = acc / float(testlen)\n",
    "    avg_loss = loss / float(testlen)\n",
    "\n",
    "    return avg_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = '/Users/dj/Desktop'\n",
    "data_name = 'cifar100'\n",
    "do_decay = True\n",
    "curvature = 1\n",
    "kwargs = {'num_workers': 32, 'pin_memory': True}\n",
    "datadir = 'data/'\n",
    "resdir = 'runs/output_dir/cifar/' \n",
    "hpnfile = 'prototypes/prototypes-50d-100c.npy' \n",
    "logdir =  'test'\n",
    "optimizer = 'adam'\n",
    "learning_rate = 0.0005\n",
    "momentum= 0.00005\n",
    "decay = True\n",
    "network = 'resnet32'\n",
    "penalty = 0.1\n",
    "mult = 0.1\n",
    "\n",
    "# I want to use tensorboard to check the loss changes\n",
    "log_dir = os.path.join('./runs/' + data_name, logdir)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Set the random seeds.\n",
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MasterThesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "batch_size = 10\n",
    "trainloader, testloader = load_dataset(dataset_name = data_name,\n",
    "        basedir = datadir,\n",
    "            batch_size = batch_size,\n",
    "                kwargs = kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the output_dims: 50\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(resdir):\n",
    "    os.makedirs(resdir)\n",
    "\n",
    "# Load the polars and update the trainy labels.\n",
    "classpolars = torch.from_numpy(np.load(hpnfile)).float()\n",
    "# calculate radius of ball\n",
    "# This part is useful when curvature is not 1.\n",
    "radius = 1.0 / math.sqrt(curvature)\n",
    "classpolars = classpolars * radius\n",
    "\n",
    "# hpnfile name is like prototypes-xd-yc.npy : x : dimension of prototype, y: number of classes\n",
    "output_dims = int(hpnfile.split(\"/\")[-1].split(\"-\")[1][:-1])\n",
    "print('This is the output_dims:', output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time model initialization.\n",
      "~~~~~~~~!Your option is not available, I am choosing!~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "if (data_name == \"cifar100\") or (data_name == \"cifar10\"):\n",
    "    if network == \"resnet32\":\n",
    "        model = resnet_cifar.ResNet(32, output_dims, 1, classpolars)\n",
    "    elif network == \"densenet121\":\n",
    "        model = densenet_cifar.DenseNet121(output_dims, classpolars)\n",
    "    else:\n",
    "        print('The model you have chosen is not available. I am choosing resnet for you.')\n",
    "        model = resnet_cifar.ResNet(32, output_dims, 1, classpolars)\n",
    "elif args.data_name == \"cub\":\n",
    "    if args.network == \"resnet32\":\n",
    "        model = resnet_cub.ResNet34(args.output_dims, classpolars)\n",
    "    else:\n",
    "        print('The model you have chosen is not available. I am choosing resnet for you.')\n",
    "        model = resnet_cub.ResNet34(args.output_dims, classpolars)\n",
    "elif args.data_name == \"syndat\":\n",
    "    if args.network == \"fullcon\":\n",
    "        print('The fully connected Network has been chosen for the Synthetic Data.')\n",
    "        model = fullcon_syndat.fullcon(dims = args.dims, output_dims = args.output_dims, dr = args.dr, polars = classpolars)\n",
    "        print('Model has been activated.')\n",
    "    elif args.network == \"fullcon_selu\":\n",
    "        print('The fully connected SELU Network has been chosen for the Synthetic Data.')\n",
    "        model = fullcon_syndat_selu.fullcon(dims = args.dims, output_dims = args.output_dims, dr = args.dr, polars = classpolars)\n",
    "        print('Model has been activated.')\n",
    "    elif args.network == \"fullcon_lrelu\":\n",
    "        print('The fully connected Leaky RELU Network has been chosen for the Synthetic Data.')\n",
    "        model = fullcon_syndat_lrelu.fullcon(dims = args.dims, output_dims = args.output_dims, dr = args.dr, polars = classpolars)\n",
    "        print('Model has been activated.')\n",
    "else:\n",
    "    raise Exception('Selected dataset is not available.')\n",
    "\n",
    "# model = model.to(device)\n",
    "print('First time model initialization.')\n",
    "\n",
    "# Load the optimizer.\n",
    "optimizer = get_optimizer(optimizer, model.parameters(), learning_rate, momentum, decay)\n",
    "\n",
    "# Initialize the loss functions.\n",
    "choose_penalty = penalty\n",
    "f_loss = PeBusePenalty(output_dims, penalty_option=choose_penalty, mult=mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean alpha only supported for Boolean results.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9r/pl8035cn4cjb59msrdwcrvhr0000gn/T/ipykernel_5747/1555288427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Train and test.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurvature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# add the train loss to the tensorboard writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9r/pl8035cn4cjb59msrdwcrvhr0000gn/T/ipykernel_5747/3590594556.py\u001b[0m in \u001b[0;36mmain_train\u001b[0;34m(model, trainloader, optimizer, initialized_loss, c)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mavgloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MasterThesis/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MasterThesis/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MasterThesis/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MasterThesis/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean alpha only supported for Boolean results."
     ]
    }
   ],
   "source": [
    "# Main loop.\n",
    "epochs = 3\n",
    "drop1 = 1\n",
    "drop2 = 2\n",
    "do_decay = True\n",
    "testscores = []\n",
    "learning_rate = learning_rate\n",
    "for i in range(epochs):\n",
    "    print(i)\n",
    "\n",
    "    # Learning rate decay.\n",
    "    if i in [drop1, drop2] and do_decay:\n",
    "        learning_rate *= 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    # Train and test.\n",
    "    acc, loss = main_train(model, trainloader, optimizer, f_loss, c=curvature)\n",
    "\n",
    "    # add the train loss to the tensorboard writer\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    writer.add_scalar(\"Accuracy/train\", acc, i)\n",
    "\n",
    "    if i != 0 and (i % 10 == 0 or i == epochs - 1): # i % 10\n",
    "        test_acc, test_loss = main_test(model, testloader, f_loss, c=curvature)\n",
    "\n",
    "        testscores.append([i, test_acc])\n",
    "\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, i)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_acc, i)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
